{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import import_ipynb\n",
    "import preprocessing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train_scaled.csv\")\n",
    "X_test  = pd.read_csv(\"X_test_scaled.csv\")\n",
    "y_train = pd.read_csv(\"y_train_scaled.csv\")[\"Loan Status\"]\n",
    "y_test  = pd.read_csv(\"y_test_scaled.csv\")[\"Loan Status\"]\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Initialize the logistic regression model\n",
    "    \n",
    "# TO DO: fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: use predict_proba to get probabilities and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate accuracy using sklearn metrics\n",
    "accuracy =\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate precision using sklearn metrics\n",
    "precision = \n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate recall using sklearn metrics\n",
    "recall = \n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate average precision using sklearn metrics\n",
    "average_precision = \n",
    "print(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f52def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate F1 score using sklearn metrics\n",
    "f1_score = \n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b598a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate specificity using confusion matrix\n",
    "\n",
    "#TO DO: Visualize confusion matrix using ConfusionMatrixDisplay\n",
    "disp = \n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Calculate ROC score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# TO DO: Calculate AUROC\n",
    "\n",
    "# TO DO: Plot ROC curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
